{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d9b498",
   "metadata": {},
   "source": [
    "# Forgotten Runes Loracle Langchain RAG Example\n",
    "\n",
    "One of the most common ways to use The Book of Lore and Wizzypedia with AI is by using RAG: Retrieval-Augmented-Generation. The idea is you search for snippets of the text in a vector database and then pass that context along with the user's chat to GPT and it answers the question.\n",
    "\n",
    "This code uses langchain and shows a basic way to query the Forgotten Runes Pinecone server and create a mini-Loracle\n",
    "\n",
    "#### Resources:\n",
    "\n",
    "- [RAG | ü¶úÔ∏èüîó Langchain](https://python.langchain.com/docs/expression_language/cookbook/retrieval) \n",
    "- [Returning sources | ü¶úÔ∏èüîó Langchain](https://python.langchain.com/docs/use_cases/question_answering/sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc53967c-6785-4d6f-806e-77907fd7b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329299b",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Setup a `.env` file with your own API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e8cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_ENV\"] = os.getenv(\"PINECONE_ENV\")\n",
    "pinecone_index_name = \"wizzypedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6497ac1",
   "metadata": {},
   "source": [
    "## Pinecone Retriever Setup\n",
    "\n",
    "Here we tell Langchain on how to fetch from Pinecone. The Pinecone database has several other fields, but we only need `text` for now. \n",
    "\n",
    "A _Retriever_ has an intermediate LLM step which will take the index definition and the question and then ask the LLM to form the best query. We then query Pinecone and return the matching documents.\n",
    "\n",
    "Using LangSmith (langchain tracing) makes this easier to understand what is going on, so I recommend you try it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d79d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"text\",\n",
    "        description=\"The detailed text or description of the item\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"A brief document from a character story or wiki page\"\n",
    "\n",
    "# define the index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index = pinecone.Index(pinecone_index_name, host=os.environ[\"PINECONE_ENV\"])\n",
    "vectorstore = Pinecone.from_existing_index(\n",
    "    index_name=pinecone_index_name, embedding=embeddings\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e1d70",
   "metadata": {},
   "source": [
    "## Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f957180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc7563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kobolds have green skin.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what color are Kobolds?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
